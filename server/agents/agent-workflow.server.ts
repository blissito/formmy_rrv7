/**
 * AgentWorkflow - Pure LlamaIndex Pattern
 * Single agent with all tools, zero custom routing logic
 * Model decides which tools to use
 */

import {
  agent,
  agentToolCallEvent,
  agentStreamEvent,
} from "@llamaindex/workflow";
import { OpenAI } from "@llamaindex/openai";
import { Anthropic } from "@llamaindex/anthropic";
import { createMemory, staticBlock } from "llamaindex";
import { getToolsForPlan, type ToolContext } from "../tools";
import type { ResolvedChatbotConfig } from "../chatbot/configResolver.server";
import { getAgentPrompt, type AgentType } from "~/utils/agents/agentPrompts";
import { getOptimalTemperature } from "../config/model-temperatures";

// Types para el workflow
interface WorkflowContext {
  userId: string;
  userPlan: string;
  chatbotId: string | null;
  message: string;
  integrations: Record<string, any>;
  resolvedConfig: ResolvedChatbotConfig;
  agentContext?: any; // Incluye conversationId para rate limiting
}

/**
 * Create LLM instance with correct provider (from agent-engine-v0)
 */
function createLLM(model: string, temperature?: number) {
  const config: any = { model };

  // Use centralized temperature resolution
  // Si no se proporciona temperature, usar la √≥ptima del modelo
  config.temperature = temperature !== undefined ? temperature : getOptimalTemperature(model);

  // üõ°Ô∏è Token limits ESTRICTOS (reducidos para evitar loops infinitos)
  if (model.startsWith("gpt-5") || model.startsWith("gpt-4")) {
    config.maxCompletionTokens = 500; // Reducido de 1000 a 500
  } else {
    config.maxTokens = 500; // Reducido de 1000 a 500
  }

  // Timeout and retries
  config.timeout = 60000;
  config.maxRetries = 3;

  // Return appropriate provider based on model
  if (model.includes("claude")) {
    config.apiKey = process.env.ANTHROPIC_API_KEY;
    return new Anthropic(config);
  } else {
    config.apiKey = process.env.OPENAI_API_KEY;
    return new OpenAI(config);
  }
}

/**
 * Mapeo transparente de modelos para performance
 */
function mapModelForPerformance(model: string): string {
  if (model === "gpt-5-nano") {
    return "gpt-4o-mini";
  }
  return model;
}

/**
 * Construye system prompt personalizado
 */
function buildSystemPrompt(
  config: ResolvedChatbotConfig,
  hasContextSearch: boolean,
  hasWebSearch: boolean,
  hasReportGeneration: boolean
): string {
  const personality = config.personality || "friendly";

  // Agent types v√°lidos
  const agentTypes: AgentType[] = ['sales', 'customer_support', 'content_seo', 'data_analyst', 'automation_ai', 'growth_hacker'];

  // üîç PRIORIDAD M√ÅXIMA: Instrucciones de b√∫squeda PRIMERO (antes de custom instructions)
  let searchInstructions = '';
  if (hasContextSearch) {
    searchInstructions = `‚ö†Ô∏è REGLA FUNDAMENTAL - SIEMPRE EJECUTAR PRIMERO:

CUANDO EL USUARIO PREGUNTA SOBRE:
- Productos, servicios, caracter√≠sticas, precios, planes, costos
- Informaci√≥n del negocio, empresa, documentaci√≥n
- Pol√≠ticas, t√©rminos, condiciones, FAQs
- CUALQUIER informaci√≥n espec√≠fica del negocio

PROTOCOLO OBLIGATORIO:
1. EJECUTAR search_context("query espec√≠fica") INMEDIATAMENTE - NO OPCIONAL
2. Si no encuentras: REFORMULAR query y BUSCAR DE NUEVO (m√≠nimo 2 intentos)
3. Si m√∫ltiples temas: EJECUTAR M√öLTIPLES B√öSQUEDAS${hasWebSearch ? `
4. Si todo falla: EJECUTAR web_search_google("${config.name === 'Ghosty' ? 'Formmy' : config.name} [tema]")
5. Solo si todo falla: "Busqu√© pero no encontr√© informaci√≥n sobre [tema]"` : `
4. Solo si todo falla: "Busqu√© en la base de conocimiento pero no encontr√© informaci√≥n sobre [tema]"`}

‚ùå PROHIBIDO ABSOLUTAMENTE:
- Responder "no tengo informaci√≥n" SIN buscar primero
- Inventar o adivinar precios, fechas, features
- Decir "no s√©" sin AGOTAR todas las b√∫squedas
- Dar informaci√≥n NO solicitada o irrelevante

üìè REGLA DE CONCISI√ìN:
- Responde SOLO lo que se pregunt√≥
- Si preguntan por UN servicio, NO enumeres TODOS
- Usa listas solo si el usuario pide m√∫ltiples opciones
- Prioriza relevancia sobre completitud

‚úÖ EJEMPLO CORRECTO:
User: "¬øTienen planes m√°s baratos que $5,000?"
‚Üí EJECUTAR: search_context("precios planes baratos econ√≥micos")
‚Üí LEER resultados
‚Üí RESPONDER: "S√≠, tenemos planes desde $3,500 para p√°ginas web..." ‚úÖ
‚Üí NO: "Te cuento sobre todos nuestros servicios: 1) P√°ginas web desde $3,500... 2) Apps desde..." ‚ùå

`;
  }

  // Construir prompt base con personalidad
  let basePrompt: string;

  // Si personality es un AgentType v√°lido, usar prompt optimizado
  if (agentTypes.includes(personality as AgentType)) {
    // NUEVO ORDEN: searchInstructions PRIMERO, luego personality y custom instructions
    basePrompt = `${searchInstructions}${config.name || "Asistente"} - ${getAgentPrompt(personality as AgentType)}${config.customInstructions ? '\n\n' + config.customInstructions : ''}`;
  } else {
    // Fallback a personalidades gen√©ricas (friendly, professional)
    const personalityMap: Record<string, string> = {
      friendly: "asistente amigable y cercano",
      professional: "asistente profesional",
    };

    // NUEVO ORDEN: searchInstructions PRIMERO
    basePrompt = `${searchInstructions}Eres ${config.name || "asistente"}, ${personalityMap[personality] || "asistente amigable"}.

${config.instructions || "Asistente √∫til."}${config.customInstructions ? '\n\n' + config.customInstructions : ''}

Usa las herramientas disponibles cuando las necesites. S√© directo y mant√©n tu personalidad.`;
  }

  // üõ°Ô∏è Restricciones de seguridad para web_search_google
  if (hasWebSearch) {
    const businessDomain = config.name === 'Ghosty' ? 'Formmy' : (config.name || "este negocio");

    basePrompt += `

üõ°Ô∏è WEB_SEARCH LIMITADO A: ${businessDomain}
SOLO b√∫squedas relacionadas con ${businessDomain}
PROHIBIDO: noticias generales, deportes, entretenimiento, temas off-topic

V√°lido: "${businessDomain} precios", "${businessDomain} features", "comparaci√≥n ${businessDomain} vs competencia"
Inv√°lido: noticias del d√≠a, deportes, celebridades

Si pregunta off-topic: "Mi b√∫squeda web est√° limitada a ${businessDomain}"`;
  }

  // üìÑ Instrucciones de reportes PDF si tiene acceso
  if (hasReportGeneration) {
    basePrompt += `

üìÑ REPORTES PDF (generate_chatbot_report):
Usa cuando usuario pida: reporte, PDF, documento, descarga, exportar

CR√çTICO:
- COPIA EXACTA del mensaje que retorna la tool
- NO modifiques el link de descarga
- NO agregues prefijos al URL (sandbox:, http:, etc)

Correcto: "‚úÖ Reporte generado... [DESCARGAR PDF](/api/ghosty/download_123)"
Incorrecto: "Descarga: sandbox:/api/ghosty/download_123"`;
  }

  return basePrompt;
}

/**
 * Crea un agente con todas las herramientas del plan + memoria conversacional
 * El modelo AI decide qu√© tools usar - zero custom routing
 *
 * ‚úÖ Patr√≥n oficial LlamaIndex TypeScript Agent Workflows:
 * 1. Crear memoria con createMemory({})
 * 2. Agregar mensajes del historial con memory.add({ role, content })
 * 3. Pasar memoria al agente en la configuraci√≥n
 *
 * Documentaci√≥n oficial: https://developers.llamaindex.ai/typescript/framework/modules/data/memory
 */
async function createSingleAgent(
  context: WorkflowContext,
  conversationHistory?: Array<{ role: "user" | "assistant"; content: string }>
) {
  const { resolvedConfig, userPlan } = context;

  // Model selection con mapping transparente
  const selectedModel = mapModelForPerformance(
    resolvedConfig.aiModel || "gpt-5-nano"
  );

  // Create LLM with correct provider (OpenAI or Anthropic)
  const llm = createLLM(selectedModel, resolvedConfig.temperature || 0.3);

  // Todas las herramientas del plan - modelo decide cu√°les usar
  const toolContext: ToolContext = {
    userId: context.userId,
    userPlan,
    chatbotId: context.chatbotId,
    conversationId: context.agentContext?.conversationId, // Para rate limiting
    message: context.message,
    integrations: context.integrations,
    isGhosty: context.chatbotId === "ghosty-main", // Ghosty tiene acceso a stats
  };

  const allTools = getToolsForPlan(userPlan, context.integrations, toolContext);

  // Detectar si tiene acceso a search_context, web_search_google, y generate_chatbot_report tools
  const hasContextSearch = allTools.some(
    (tool: any) => tool.metadata?.name === "search_context"
  );
  const hasWebSearch = allTools.some(
    (tool: any) => tool.metadata?.name === "web_search_google"
  );
  const hasReportGeneration = allTools.some(
    (tool: any) => tool.metadata?.name === "generate_chatbot_report"
  );

  const systemPrompt = buildSystemPrompt(resolvedConfig, hasContextSearch, hasWebSearch, hasReportGeneration);

  // ‚úÖ Crear memoria conversacional seg√∫n patr√≥n oficial LlamaIndex
  let memory = undefined;

  console.log(`\n${'='.repeat(80)}`);
  console.log(`üß† [createSingleAgent] INICIANDO CREACI√ìN DE MEMORIA`);
  console.log(`   Historial recibido: ${conversationHistory ? conversationHistory.length : 0} mensajes`);
  console.log(`${'='.repeat(80)}\n`);

  if (conversationHistory && conversationHistory.length > 0) {
    console.log(`üß† Historial NO est√° vac√≠o - procediendo a crear memoria con staticBlock`);

    // üîß Formatear historial como texto para staticBlock
    const historyText = conversationHistory.map((msg) => {
      const roleLabel = msg.role === 'user' ? 'Usuario' : msg.role === 'assistant' ? 'Asistente' : 'Sistema';
      return `${roleLabel}: ${msg.content}`;
    }).join('\n\n');

    console.log(`üìù Historial formateado para staticBlock:\n${historyText.substring(0, 200)}...\n`);

    // üöÄ Crear memoria con staticBlock (patr√≥n oficial LlamaIndex)
    memory = createMemory({
      tokenLimit: 8000,
      memoryBlocks: [
        staticBlock({
          content: `Historial de la conversaci√≥n:\n\n${historyText}`,
        }),
      ],
    });

    console.log(`‚úÖ Memoria creada con staticBlock`);
    console.log(`   ${conversationHistory.length} mensajes en el bloque est√°tico`);
  } else {
    console.log(`‚ö†Ô∏è  [createSingleAgent] Sin historial - memoria NO creada`);
    console.log(`   conversationHistory: ${conversationHistory}`);
    console.log(`   length: ${conversationHistory?.length}`);
  }

  // ‚úÖ Patr√≥n oficial LlamaIndex TypeScript: pasar memoria en configuraci√≥n del agente
  const agentConfig: any = {
    llm,
    tools: allTools,
    systemPrompt,
    verbose: true, // Para debugging de herramientas
  };

  console.log(`\n${'='.repeat(80)}`);
  console.log(`üéØ [createSingleAgent] CONFIGURANDO AGENTE`);
  console.log(`   LLM: ${llm.model || llm.constructor.name}`);
  console.log(`   Tools: ${allTools.length}`);
  console.log(`   System prompt: ${systemPrompt.substring(0, 80)}...`);
  console.log(`${'='.repeat(80)}\n`);

  // Solo agregar memoria si existe
  if (memory) {
    agentConfig.memory = memory;
    console.log(`‚úÖ MEMORIA AGREGADA A AGENT CONFIG (staticBlock)`);
  } else {
    console.log(`‚ö†Ô∏è  SIN MEMORIA - agente NO recordar√° conversaciones previas`);
  }

  console.log(`\n${'='.repeat(80)}`);
  console.log(`üöÄ Llamando a agent() con config...`);
  console.log(`${'='.repeat(80)}\n`);

  return agent(agentConfig);
}

/**
 * Definici√≥n de cr√©ditos por herramienta seg√∫n pricing CLAUDE.md
 */
const TOOL_CREDITS: Record<string, number> = {
  // B√°sicas (1 cr√©dito)
  'save_contact_info': 1,
  'get_current_datetime': 1,

  // Intermedias (2-3 cr√©ditos)
  'schedule_reminder': 2,
  'list_reminders': 2,
  'update_reminder': 2,
  'cancel_reminder': 2,
  'delete_reminder': 2,
  'web_search_google': 3,
  'search_context': 2,

  // Avanzadas (4-6 cr√©ditos)
  'create_payment_link': 4,
  'get_usage_limits': 2,
  'query_chatbots': 3,
  'get_chatbot_stats': 5,
};

/**
 * Stream de un agente con tracking de eventos
 * El historial conversacional ya est√° en el agente via memoria (memory)
 *
 * ‚úÖ Patr√≥n oficial LlamaIndex: el agente mantiene el contexto completo autom√°ticamente
 * mediante el sistema de memoria integrado
 *
 * üõ°Ô∏è PROTECCIONES DE SEGURIDAD:
 * - Timeout m√°ximo: 45 segundos
 * - M√°ximo 1000 chunks
 * - Detecci√≥n de contenido corrupto
 */
async function* streamSingleAgent(agentInstance: any, message: string, availableTools: string[] = []) {
  const MAX_CHUNKS = 1000;
  const MAX_DURATION_MS = 45000; // 45 segundos
  const startTime = Date.now();

  // El agente ya tiene memoria configurada con el historial, solo pasamos el mensaje actual
  const events = agentInstance.runStream(message);

  let hasStreamedContent = false;
  let toolsExecuted = 0;
  let toolsUsed: string[] = [];
  let chunkCount = 0;
  let totalChars = 0;
  let totalTokens = 0; // Tracking de tokens
  let creditsUsed = 0; // Tracking de cr√©ditos

  try {
    for await (const event of events as any) {
      // üõ°Ô∏è PROTECCI√ìN 1: Timeout
      const elapsed = Date.now() - startTime;
      if (elapsed > MAX_DURATION_MS) {
        console.error(`‚è±Ô∏è Stream timeout despu√©s de ${elapsed}ms`);
        yield {
          type: "error",
          content:
            "‚è±Ô∏è Respuesta demasiado larga. Por favor intenta reformular tu pregunta.",
        };
        break;
      }

      // üõ°Ô∏è PROTECCI√ìN 2: M√°ximo de chunks
      if (chunkCount >= MAX_CHUNKS) {
        console.error(`üö´ M√°ximo de chunks alcanzado: ${chunkCount}`);
        yield {
          type: "error",
          content:
            "La respuesta es demasiado extensa. Por favor s√© m√°s espec√≠fico.",
        };
        break;
      }

      // Tool call events
      if (agentToolCallEvent.include(event)) {
        toolsExecuted++;
        const toolName = event.data.toolName || "unknown_tool";
        toolsUsed.push(toolName);

        // Calcular cr√©ditos consumidos por esta tool
        const toolCredits = TOOL_CREDITS[toolName] || 1; // Default 1 cr√©dito
        creditsUsed += toolCredits;

        yield {
          type: "tool-start",
          tool: toolName,
          message: `üîß ${toolName}`,
        };
      }

      // Stream content events
      if (agentStreamEvent.include(event)) {
        if (event.data.delta) {
          chunkCount++;
          totalChars += event.data.delta.length;

          // Estimar tokens basado en caracteres (regla aproximada: ~4 chars por token)
          totalTokens += Math.ceil(event.data.delta.length / 4);

          // üõ°Ô∏è PROTECCI√ìN 3: Detectar contenido corrupto (m√∫ltiples scripts)
          const hasMultipleScripts =
            /[\u0400-\u04FF].*[\u0E00-\u0E7F]|[\u0600-\u06FF].*[\u4E00-\u9FFF]|[\u0900-\u097F].*[\u0400-\u04FF]/.test(
              event.data.delta
            );
          if (hasMultipleScripts && event.data.delta.length > 100) {
            console.error(
              `üö´ Contenido corrupto detectado en chunk ${chunkCount}`
            );
            yield {
              type: "error",
              content:
                "Error de generaci√≥n detectado. Por favor intenta de nuevo.",
            };
            break;
          }

          hasStreamedContent = true;
          yield {
            type: "chunk",
            content: event.data.delta,
          };
        }
      }
    }
  } catch (streamError) {
    console.error(`‚ùå Error en streaming:`, streamError);
    yield {
      type: "error",
      content: "Error durante la generaci√≥n de respuesta.",
    };
  }

  // Fallback para casos donde tools ejecutan pero no stream
  if (toolsExecuted > 0 && !hasStreamedContent) {
    yield {
      type: "chunk",
      content: "He ejecutado las herramientas solicitadas correctamente.",
    };
  }

  yield {
    type: "done",
    metadata: {
      toolsExecuted,
      toolsUsed,
      availableTools, // üîß Lista de herramientas disponibles para el usuario
      tokensUsed: totalTokens,
      creditsUsed,
      estimatedCost: {
        tokens: totalTokens,
        credits: creditsUsed,
        // Estimaci√≥n aproximada: GPT-4o-mini = $0.15 input + $0.60 output por 1M tokens
        // Asumiendo 50/50 input/output para simplificar
        usdCost: ((totalTokens / 1_000_000) * 0.375).toFixed(6)
      }
    },
  };
}

/**
 * AgentWorkflow principal - Simplified con memoria conversacional
 */
export const streamAgentWorkflow = async function* (
  user: any,
  message: string,
  chatbotId?: string,
  options: {
    resolvedConfig: ResolvedChatbotConfig;
    agentContext: any;
  } = {} as any
) {
  const context: WorkflowContext = {
    userId: user.id,
    userPlan: user.plan || "FREE",
    chatbotId: chatbotId || null,
    message,
    integrations: options.agentContext?.integrations || {},
    resolvedConfig: options.resolvedConfig,
    agentContext: options.agentContext, // Incluir agentContext completo
  };

  try {
    // Extraer historial conversacional del agentContext
    const conversationHistory = options.agentContext?.conversationHistory || [];

    console.log(`\n${'üî•'.repeat(40)}`);
    console.log(`üöÄ [streamAgentWorkflow] INICIO`);
    console.log(`   agentContext recibido: ${!!options.agentContext}`);
    console.log(`   conversationHistory extra√≠do: ${conversationHistory.length} mensajes`);

    if (conversationHistory.length > 0) {
      console.log(`\n   üìö HISTORIAL EXTRA√çDO DEL AGENTCONTEXT:`);
      conversationHistory.forEach((msg, i) => {
        console.log(`   [${i + 1}] ${msg.role}: "${msg.content.substring(0, 60)}..."`);
      });
    } else {
      console.log(`   ‚ö†Ô∏è  conversationHistory est√° VAC√çO`);
      console.log(`   agentContext completo:`, JSON.stringify(options.agentContext, null, 2));
    }
    console.log(`${'üî•'.repeat(40)}\n`);

    // Single agent con todas las tools + memoria conversacional
    const agentInstance = await createSingleAgent(context, conversationHistory);

    // üîß Obtener lista de herramientas disponibles para el usuario
    const toolContext: ToolContext = {
      userId: context.userId,
      userPlan: context.userPlan,
      chatbotId: context.chatbotId,
      conversationId: context.agentContext?.conversationId,
      message: context.message,
      integrations: context.integrations,
      isGhosty: context.chatbotId === "ghosty-main",
    };
    const availableToolsObjects = getToolsForPlan(context.userPlan, context.integrations, toolContext);
    const availableTools = availableToolsObjects.map((tool: any) => tool.metadata?.name || 'unknown');

    // Stream con memoria ya configurada en el agente + lista de tools disponibles
    yield* streamSingleAgent(agentInstance, message, availableTools);
  } catch (error) {
    console.error("‚ùå AgentWorkflow error:", error);
    yield {
      type: "error",
      content:
        "Lo siento, hubo un problema procesando tu mensaje. Por favor intenta de nuevo.",
    };
  }
};

/**
 * Cache management (placeholder para futuro)
 */
const agentCache = new Map<string, any>();

export const clearAgentWorkflowCache = () => {
  agentCache.clear();
};

export const getAgentWorkflowStats = () => {
  return {
    cachedAgents: agentCache.size,
  };
};
